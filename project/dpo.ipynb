{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14b878b9-713c-4f5e-95b4-92ca1360f01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zhenwu/11868/llm_sys_project\n"
     ]
    }
   ],
   "source": [
    "%cd /home/zhenwu/11868/llm_sys_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9c31dec-3f0b-487f-b199-9a40608c53cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import datasets\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import defaultdict\n",
    "# import tqdm\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import random\n",
    "# from bs4 import BeautifulSoup, NavigableString\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Iterator, Callable, Union, Tuple\n",
    "import os\n",
    "import transformers\n",
    "\n",
    "import minitorch\n",
    "from minitorch import DecoderLM\n",
    "from minitorch.tensor import *\n",
    "from minitorch.tensor_functions import *\n",
    "from minitorch.nn import *\n",
    "from minitorch.cuda_kernel_ops import CudaKernelOps\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa356d57-0bcd-4b9a-8001-26bf6d2fdb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imdb(data_path: str = 'data/imdb.json', split: str = None, silent: bool = False, cache_dir: str = None) -> Dict[str, Dict[str, Union[List[Tuple[int, int]], List[str], str]]]:\n",
    "    \"\"\"Load the Anthropic Helpful-Harmless dataset from Huggingface and convert it to the necessary format.\n",
    "       For this dataset, the sft_target is just the chosen response.\n",
    "    \"\"\"\n",
    "    print(f'Loading IMDB RLHF dataset...')\n",
    "    dataset = datasets.load_dataset(\"json\", data_files=\"/home/zhenwu/11868/llm_sys_project/data/imdb.json\")\n",
    "    print('done')\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b34259bc-fc02-4d3d-8ce0-95edee938423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_batch_element(prompt: str, chosen: str, rejected: str, truncation_mode: str, tokenizer, max_length: int, max_prompt_length: int) -> Dict:\n",
    "    \"\"\"Tokenize a single batch element.\n",
    "    \n",
    "       At this stage, we don't convert to PyTorch tensors yet; we just handle the truncation\n",
    "         in case the prompt + chosen or prompt + rejected responses is/are too long. First\n",
    "         we truncate the prompt; if we're still too long, we truncate the chosen/rejected.\n",
    "       \n",
    "       We also create the labels for the chosen/rejected responses, which are of length equal to\n",
    "         the sum of the length of the prompt and the chosen/rejected response, with -100 for the\n",
    "         prompt tokens.\n",
    "    \"\"\"\n",
    "    chosen_tokens = tokenizer(chosen, add_special_tokens=False)\n",
    "    rejected_tokens = tokenizer(rejected, add_special_tokens=False)\n",
    "    prompt_tokens = tokenizer(prompt, add_special_tokens=False)\n",
    "\n",
    "    # assert tokenizer.eos_token_id not in prompt_tokens['input_ids'], f\"Prompt contains EOS token: {prompt}\"\n",
    "    # assert tokenizer.eos_token_id not in chosen_tokens['input_ids'], f\"Chosen response contains EOS token: {chosen}\"\n",
    "    # assert tokenizer.eos_token_id not in rejected_tokens['input_ids'], f\"Rejected response contains EOS token: {rejected}\"\n",
    "\n",
    "    if tokenizer.eos_token_id not in chosen_tokens['input_ids']:\n",
    "        chosen_tokens['input_ids'].append(tokenizer.eos_token_id)\n",
    "        chosen_tokens['attention_mask'].append(1)\n",
    "    # chosen_tokens['input_ids'].append(tokenizer.eos_token_id)\n",
    "    # chosen_tokens['attention_mask'].append(1)\n",
    "\n",
    "    if tokenizer.eos_token_id not in rejected_tokens['input_ids']:\n",
    "        rejected_tokens['input_ids'].append(tokenizer.eos_token_id)\n",
    "        rejected_tokens['attention_mask'].append(1)\n",
    "\n",
    "    # rejected_tokens['input_ids'].append(tokenizer.eos_token_id)\n",
    "    # rejected_tokens['attention_mask'].append(1)\n",
    "\n",
    "    if len(chosen_tokens['input_ids']) - len(rejected_tokens['input_ids']) > 0:\n",
    "        longer_response_length = len(chosen_tokens['input_ids'])\n",
    "\n",
    "    else:\n",
    "        longer_response_length = len(rejected_tokens['input_ids'])\n",
    "\n",
    "    # longer_response_length = max(len(chosen_tokens['input_ids']), len(rejected_tokens['input_ids']))\n",
    "\n",
    "    # if combined sequence is too long, truncate the prompt\n",
    "    if len(prompt_tokens['input_ids']) + longer_response_length > max_length:\n",
    "        if truncation_mode == 'keep_start':\n",
    "            prompt_tokens = {k: v[:max_prompt_length] for k, v in prompt_tokens.items()}\n",
    "        elif truncation_mode == 'keep_end':\n",
    "            prompt_tokens = {k: v[-max_prompt_length:] for k, v in prompt_tokens.items()}\n",
    "        else:\n",
    "            raise ValueError(f'Unknown truncation mode: {truncation_mode}')\n",
    "\n",
    "    # if that's still too long, truncate the response\n",
    "    if len(prompt_tokens['input_ids']) + longer_response_length > max_length:\n",
    "        chosen_tokens = {k: v[:max_length - max_prompt_length] for k, v in chosen_tokens.items()}\n",
    "        rejected_tokens = {k: v[:max_length - max_prompt_length] for k, v in rejected_tokens.items()}\n",
    "\n",
    "    # Create labels\n",
    "    chosen_sequence_tokens = {k: prompt_tokens[k] + chosen_tokens[k] for k in chosen_tokens}\n",
    "    rejected_sequence_tokens = {k: prompt_tokens[k] + rejected_tokens[k] for k in rejected_tokens}\n",
    "    chosen_sequence_tokens['labels'] = chosen_sequence_tokens['input_ids'][:]\n",
    "    chosen_sequence_tokens['labels'][:len(prompt_tokens['input_ids'])] = [0] * len(prompt_tokens['input_ids'])\n",
    "    rejected_sequence_tokens['labels'] = rejected_sequence_tokens['input_ids'][:]\n",
    "    rejected_sequence_tokens['labels'][:len(prompt_tokens['input_ids'])] = [0] * len(prompt_tokens['input_ids'])\n",
    "\n",
    "    # print(chosen_sequence_tokens['labels'])\n",
    "    \n",
    "    batch = {}\n",
    "\n",
    "    batch['prompt'] = prompt\n",
    "    batch['chosen'] = prompt + chosen\n",
    "    batch['rejected'] = prompt + rejected\n",
    "    batch['chosen_response_only'] = chosen\n",
    "    batch['rejected_response_only'] = rejected\n",
    "\n",
    "    for k, toks in {'chosen': chosen_sequence_tokens, 'rejected': rejected_sequence_tokens, 'prompt': prompt_tokens}.items():\n",
    "        for type_key, tokens in toks.items():\n",
    "            if type_key == 'token_type_ids':\n",
    "                continue\n",
    "            batch[f'{k}_{type_key}'] = tokens\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5073aac1-5f03-462d-8760-1c1cf3220296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(\n",
    "        examples, tokenizer, model_max_length, backend=None):\n",
    "    \"\"\"\n",
    "    Prepares a batch of examples for model training or evaluation by tokenizing and padding them.\n",
    "\n",
    "    Parameters:\n",
    "    - examples: A list of examples to be processed.\n",
    "    - src_key: The key for accessing source texts in the examples.\n",
    "    - tgt_key: The key for accessing target texts in the examples.\n",
    "    - tokenizer: The tokenizer to be used for encoding the texts.\n",
    "    - model_max_length: The maximum sequence length the model can handle.\n",
    "    - backend: The backend of minitorch tensors.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing keys: 'input_ids', 'labels', 'label_token_weights',\n",
    "        each indicates a minitorch tensor with shape (len(examples), model_max_length).\n",
    "\n",
    "    Notes:\n",
    "    [\"input_ids\"] for every example in the DE-EN translation, the \"input_ids\" will be:\n",
    "        <de_token_ids> + <de_eos_id> + <en_token_ids> + <en_eos_id> + <pad_ids>\n",
    "    where the pad_ids makes the length of input_ids to be model_max_length.\n",
    "\n",
    "    [\"labels\"]: the next tokens to be predicted, which will be used in the cross-entropy\n",
    "    loss function, e.g., for an example tokenized as [a, b, c, d], \"input_ids\" and \"labels\" \n",
    "    can be [a, b, c] and [b, c, d], respectively.\n",
    "\n",
    "    [\"label_token_weights\"] The 'label_token_weights' are used to differentiate\n",
    "    calculation purposes. (the MLE loss is computed on target tokens only.)\n",
    "    between the source (weight = 0) and target (weight = 1) tokens for loss\n",
    "\n",
    "    TODO: \n",
    "        outputs: [chosen token ids, prompt token ids, rejected token ids]\n",
    "    \n",
    "    \"\"\"\n",
    "    token_ids_chosen, token_masks_chosen, token_ids_rejected, token_masks_rejected = [], [], [], []\n",
    "    label_ids_chosen, label_ids_rejected = [], []\n",
    "    # pad_token_id = tokenizer.vocab['<pad>']\n",
    "    pad_token_id = tokenizer.encode('<pad>')[1]\n",
    "    # print(pad_token_id)\n",
    "    for i in range(len(examples['prompt'])):\n",
    "        # print('example: ', example)\n",
    "        batch = tokenize_batch_element(examples['prompt'][i], examples['chosen'][i], examples['rejected'][i], 'keep_start', tokenizer, 512, 256)\n",
    "\n",
    "        # ------------ToDo------------\n",
    "        # token_ids_chosen = tokenizer(\n",
    "        #    f'{example[chosen_key]}<eos_{chosen_key}>')['input_ids']\n",
    "        # ------------ToDo------------\n",
    "\n",
    "        # BEGIN ASSIGN2_2\n",
    "        # TODO\n",
    "        # create token_ids, labels, and label_token_weights for every example\n",
    "        # hint: based on token_ids_src, token_ids_tgt, and pad_token_id\n",
    "        \n",
    "        # input_ids is <de_token_ids> + <de_eos_id> + <en_token_ids> + <en_eos_id> + <pad_ids> where \n",
    "        # the pad_ids makes the length of input_ids to be model_max_length\n",
    "\n",
    "        # total_len = len(token_ids_src) + len(token_ids_tgt)\n",
    "        # token_pad_length = model_max_length - total_len\n",
    "\n",
    "        total_len_chosen = len(batch['chosen_input_ids'])\n",
    "        token_pad_length_chosen = model_max_length - total_len_chosen\n",
    "        \n",
    "        \n",
    "        \n",
    "        total_len_rejected = len(batch['rejected_input_ids'])\n",
    "        token_pad_length_rejected = model_max_length - total_len_rejected\n",
    "        \n",
    "        # pad \n",
    "        token_id_chosen = batch['chosen_input_ids'] + [pad_token_id] * token_pad_length_chosen\n",
    "        # print('token_id_chosen: ', len(batch['chosen_input_ids']))\n",
    "        token_mask_chosen = [1] * len(batch['chosen_input_ids']) + [0] * token_pad_length_chosen\n",
    "        label_id_chosen = batch['chosen_labels'] + [-0] * token_pad_length_chosen\n",
    "        token_id_chosen = token_id_chosen[:model_max_length]\n",
    "        token_mask_chosen = token_mask_chosen[:model_max_length]\n",
    "        \n",
    "        token_ids_chosen.append(token_id_chosen)\n",
    "        token_masks_chosen.append(token_mask_chosen)\n",
    "        label_ids_chosen.append(label_id_chosen)\n",
    "        \n",
    "        token_id_rejected = batch['rejected_input_ids'] + [pad_token_id] * token_pad_length_rejected\n",
    "        token_mask_rejected = [1] * len(batch['rejected_input_ids']) + [0] * token_pad_length_rejected\n",
    "        label_id_rejected = batch['rejected_labels'] + [0] * token_pad_length_rejected\n",
    "        token_id_rejected = token_id_rejected[:model_max_length]\n",
    "        token_mask_rejected = token_mask_rejected[:model_max_length]\n",
    "        \n",
    "        token_ids_rejected.append(token_id_rejected)\n",
    "        token_masks_rejected.append(token_mask_rejected)\n",
    "        label_ids_rejected.append(label_id_rejected)\n",
    "        \n",
    "        assert len(token_id_chosen) == model_max_length, print(len(token_id_chosen), total_len_chosen, token_pad_length_chosen)\n",
    "    \n",
    "\n",
    "    return {\n",
    "        'chosen_input_ids': minitorch.tensor_functions.tensor_from_numpy(np.array(token_ids_chosen), backend),\n",
    "        # 'chosen_masks': minitorch.tensor_functions.tensor_from_numpy(np.array(token_masks_chosen), backend),\n",
    "        'chosen_labels': minitorch.tensor_functions.tensor_from_numpy(np.array(label_ids_chosen), backend),\n",
    "        'rejected_input_ids': minitorch.tensor_functions.tensor_from_numpy(np.array(token_ids_rejected), backend),\n",
    "        # 'rejected_masks': minitorch.tensor_functions.tensor_from_numpy(np.array(token_masks_rejected), backend),\n",
    "        'rejected_labels': minitorch.tensor_functions.tensor_from_numpy(np.array(label_ids_rejected), backend),\n",
    "        # 'concatenated_input_ids': minitorch.tensor_functions.tensor_from_numpy(np.concatenate((np.array(token_ids_chosen), np.array(token_ids_rejected))), backend),\n",
    "        # 'concatenated_masks': minitorch.tensor_functions.tensor_from_numpy(np.concatenate((np.array(token_masks_chosen), np.array(token_masks_rejected))), backend),\n",
    "        # 'concatenated_labels': minitorch.tensor_functions.tensor_from_numpy(np.concatenate((np.array(label_ids_chosen), np.array(label_ids_rejected))), backend)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0791bdf-23de-4eab-8e19-1772c341c1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather(input, dim, index):\n",
    "    output = index.zeros(shape=index.shape)  # Assuming Minitorch has a method to create a zero tensor\n",
    "\n",
    "    # We use nested loops to iterate over each dimension except the one we are gathering from\n",
    "    if dim == 2:  # As per your use case, gathering from the last dimension\n",
    "        for i in range(input.shape[0]):  # Loop over the first dimension (batch size)\n",
    "            for j in range(input.shape[1]):  # Loop over the second dimension (sequence length)\n",
    "                k = int(index[i, j])  # This is the index in the vocab size dimension\n",
    "                output[i, j] = input[i, j, k]\n",
    "    else:\n",
    "        raise NotImplementedError(\"Gather function only implemented for dim=2\")\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8590a28d-f370-46aa-8cd7-869dcf12f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tensor(tensor, split_index):\n",
    "    # Assuming tensor has shape [N] and split_index is the point to split\n",
    "    part1 = tensor.zeros((split_index, 1))  # Tensor for the first half, full dimensionality\n",
    "    part2 = tensor.zeros((tensor.shape[0] - split_index, 1))\n",
    "    for i in range(tensor.shape[0]):\n",
    "        if i < split_index:\n",
    "            part1[i, 0] = tensor[i, 0]  # Manually set each element\n",
    "        else:\n",
    "            part2[i - split_index, 0] = tensor[i, 0]  # Adjust index for second part\n",
    "\n",
    "    return part1, part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ece1bb6-85b4-456c-89e3-1d84dbb64d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(batch, model, backend):\n",
    "    \"\"\"\n",
    "    The MLE loss for a batch with separate chosen and rejected components.\n",
    "\n",
    "    Parameters:\n",
    "    - batch: The result of collate_fn, a dict with separate \"chosen_input_ids\", \"rejected_input_ids\", \"chosen_labels\", \"rejected_labels\".\n",
    "    - model: The model to be trained.\n",
    "\n",
    "    Returns:\n",
    "    - A scalar loss value for this batch, averaged across all target tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # Process chosen and rejected parts separately\n",
    "    chosen_idx = batch['chosen_input_ids']\n",
    "    chosen_labels = batch['chosen_labels']\n",
    "    chosen_idx.requires_grad_(True)  # Ensure gradients are tracked\n",
    "    \n",
    "    rejected_idx = batch['rejected_input_ids']\n",
    "    rejected_labels = batch['rejected_labels']\n",
    "    rejected_idx.requires_grad_(True)\n",
    "\n",
    "    # Generate logits for both parts\n",
    "    chosen_logits = model(idx=chosen_idx)\n",
    "    rejected_logits = model(idx=rejected_idx)\n",
    "\n",
    "    # Apply log softmax and gather operations, assuming you need these based on previous logic\n",
    "    chosen_logps = gather(logsoftmax(chosen_logits, -1), 2, chosen_labels)\n",
    "    rejected_logps = gather(logsoftmax(rejected_logits, -1), 2, rejected_labels)\n",
    "\n",
    "    # Apply masks if there are any labels to mask\n",
    "    chosen_loss_mask = chosen_labels != 0\n",
    "    rejected_loss_mask = rejected_labels != 0\n",
    "\n",
    "    # Sum the log probabilities over the sequence length\n",
    "    chosen_batch_logps = (chosen_logps * chosen_loss_mask).sum(-1)\n",
    "    rejected_batch_logps = (rejected_logps * rejected_loss_mask).sum(-1)\n",
    "\n",
    "    # Compute the preference loss directly using the separated logits\n",
    "    loss, chosen_rewards, rejected_rewards = minitorch.nn.preference_loss(\n",
    "        chosen_batch_logps, rejected_batch_logps, beta=0.7\n",
    "    )\n",
    "\n",
    "    # Sum up the loss to a scalar for backward pass\n",
    "    batch_loss = loss.sum()\n",
    "\n",
    "    print('loss: ', batch_loss)\n",
    "    print(\"Gradients tracking on chosen_batch_logps:\", chosen_batch_logps.requires_grad)\n",
    "    print(\"Gradients tracking on rejected_batch_logps:\", rejected_batch_logps.requires_grad)\n",
    "\n",
    "    return batch_loss, chosen_rewards, rejected_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6741bc4-3391-48b9-959f-df904c644b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, examples, n_samples, collate_fn, batch_size, desc, backend):\n",
    "    \"\"\"\n",
    "    Trains the model on the provided examples.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The model to be trained.\n",
    "    - optimizer: The optimizer used for updating the model's parameters.\n",
    "    - examples: The dataset examples used for training.\n",
    "    - n_samples: The random samples to train from \"examples\".\n",
    "    - collate_fn: The function to collate data examples into batches.\n",
    "    - batch_size: The number of examples in each batch.\n",
    "    - desc: Description for the training process (used in progress bars).\n",
    "\n",
    "    # ------------ToDo------------\n",
    "    add preference policy model\n",
    "    # ------------ToDo------------\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    # random.shuffle(examples)\n",
    "    # examples = examples[:n_samples]\n",
    "\n",
    "    for i in range(0, len(examples), batch_size):\n",
    "        batch = collate_fn(examples=examples[i:i + batch_size])\n",
    "\n",
    "        t0 = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss, _, _ = loss_fn(batch=batch, model=model, backend=backend)\n",
    "        t1 = time.time()\n",
    "\n",
    "        loss.backward()\n",
    "        print(loss.grad)\n",
    "        t2 = time.time()\n",
    "\n",
    "        optimizer.step()\n",
    "        t3 = time.time()\n",
    "\n",
    "        # print(f\"Forward: {t1 - t0}\")\n",
    "        # print(f\"Backward: {t2 - t1}\")\n",
    "        # print(f\"Opt.step: {t3 - t2}\")\n",
    "\n",
    "        batch_time = time.time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe3f925e-b555-4be3-9c9f-2f35ddbb5f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model_max_length=512,\n",
    "         n_epochs=2,\n",
    "         batch_size=10,\n",
    "         learning_rate=0.02,\n",
    "         samples_per_epoch=2,\n",
    "         n_vocab=10000,\n",
    "         n_embd=256,\n",
    "         seed=11111):\n",
    "    \"\"\"\n",
    "    The main function to train and evaluate the model on a specified dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_name: The name of the dataset to be used.\n",
    "    - model_max_length: The maximum sequence length the model can handle.\n",
    "    - n_epochs: The number of training epochs.\n",
    "    - batch_size: The number of examples in each batch.\n",
    "    - learning_rate: The learning rate for the optimizer.\n",
    "    - samples_per_epoch: Samples from the training dataset every epoch.\n",
    "    - n_vocab: The vocabulary size of the BPE tokenizer.\n",
    "    - n_embd: The embedding dimension.\n",
    "    - seed: Random seed.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # workdir = f'./workdir_vocab{n_vocab}_lr{learning_rate}_embd{n_embd}'\n",
    "    # os.makedirs(workdir, exist_ok=True)\n",
    "\n",
    "    backend = minitorch.TensorBackend(CudaKernelOps)\n",
    "\n",
    "    config = {\n",
    "        'n_vocab': 50257,  # vocab_size\n",
    "        'n_embd': 256,  # n_embed\n",
    "        'n_head': 8,  # n_head\n",
    "        'n_positions': 512,  # n_ctx == n_positions\n",
    "        # 'n_layer'     : 4,    # n_layer\n",
    "        'p_dropout': 0.1,  # x_pdrop\n",
    "        'ln_eps': 1e-5,  # layer_norm_epsilon\n",
    "        'backend': backend\n",
    "    }\n",
    "\n",
    "    model = DecoderLM(**config)\n",
    "    optimizer = minitorch.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    dataset = get_imdb()\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2-large', special_tokens={'pad_token': '<pad>'})\n",
    "\n",
    "    collate_fn = partial(collate_batch, tokenizer=tokenizer, model_max_length=512, backend=backend)\n",
    "\n",
    "    for epoch_idx in range(n_epochs):\n",
    "        desc = f'epoch {epoch_idx} / {n_epochs}'\n",
    "\n",
    "        train(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            examples=dataset['train'],\n",
    "            n_samples=samples_per_epoch,\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=collate_fn,\n",
    "            desc=desc,\n",
    "            backend=backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78711035-4461-4da6-a139-99c4dad1f09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB RLHF dataset...\n",
      "done\n",
      "loss:  \n",
      "[23.025854]\n",
      "Gradients tracking on chosen_batch_logps: <bound method Tensor.requires_grad of \n",
      "[\n",
      "\t[0.000000]\n",
      "\t[0.000000]\n",
      "\t[0.000000]\n",
      "\t[0.000000]\n",
      "\t[0.000000]\n",
      "\t[0.000000]\n",
      "\t[0.000000]\n",
      "\t[0.000000]\n",
      "\t[0.000000]\n",
      "\t[0.000000]]>\n",
      "Gradients tracking on rejected_batch_logps: <bound method Tensor.requires_grad of \n",
      "[\n",
      "\t[0.000000]\n",
      "\t[0.000000]\n",
      "\t[0.000000]\n",
      "\t[0.000000]\n",
      "\t[0.000000]\n",
      "\t[0.000000]\n",
      "\t[0.000000]\n",
      "\t[0.000000]\n",
      "\t[0.000000]\n",
      "\t[0.000000]]>\n",
      "None\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 54\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(model_max_length, n_epochs, batch_size, learning_rate, samples_per_epoch, n_vocab, n_embd, seed)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m     52\u001b[0m     desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 54\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msamples_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, examples, n_samples, collate_fn, batch_size, desc, backend)\u001b[0m\n\u001b[1;32m     25\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 27\u001b[0m loss, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     30\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(batch, model, backend)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Generate logits for both parts\u001b[39;00m\n\u001b[1;32m     23\u001b[0m chosen_logits \u001b[38;5;241m=\u001b[39m model(idx\u001b[38;5;241m=\u001b[39mchosen_idx)\n\u001b[0;32m---> 24\u001b[0m rejected_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrejected_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Apply log softmax and gather operations, assuming you need these based on previous logic\u001b[39;00m\n\u001b[1;32m     27\u001b[0m chosen_logps \u001b[38;5;241m=\u001b[39m gather(logsoftmax(chosen_logits, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m2\u001b[39m, chosen_labels)\n",
      "File \u001b[0;32m~/11868/llm_sys_project/minitorch/module.py:108\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/11868/llm_sys_project/minitorch/modules_transfomer.py:356\u001b[0m, in \u001b[0;36mDecoderLM.forward\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m layer5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln(layer4)\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# Get correct shape\u001b[39;00m\n\u001b[0;32m--> 356\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mview(batch_size, seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_vocab)\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/11868/llm_sys_project/minitorch/module.py:108\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/11868/llm_sys_project/minitorch/modules_basic.py:151\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    147\u001b[0m batch, in_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m### BEGIN YOUR SOLUTION\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# apply linear transformation\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     output \u001b[38;5;241m=\u001b[39m output \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/11868/llm_sys_project/minitorch/tensor.py:171\u001b[0m, in \u001b[0;36mTensor.__matmul__\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__matmul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, b: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot used until Module 3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/11868/llm_sys_project/minitorch/tensor_functions.py:59\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *vals)\u001b[0m\n\u001b[1;32m     56\u001b[0m ctx \u001b[38;5;241m=\u001b[39m Context(\u001b[38;5;129;01mnot\u001b[39;00m need_grad)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Call forward with the variables.\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mraw_vals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# assert isinstance(c, Tensor), \"Expected return type Tensor got %s\" % (\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m#     type(c)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Create a new variable from the result with a new history.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m back \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/11868/llm_sys_project/minitorch/tensor_functions.py:43\u001b[0m, in \u001b[0;36mFunction._forward\u001b[0;34m(cls, ctx, *inps)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward\u001b[39m(\u001b[38;5;28mcls\u001b[39m, ctx: Context, \u001b[38;5;241m*\u001b[39minps: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/11868/llm_sys_project/minitorch/tensor_functions.py:429\u001b[0m, in \u001b[0;36mMatMul.forward\u001b[0;34m(ctx, t1, t2)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(ctx: Context, t1: Tensor, t2: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    428\u001b[0m     ctx\u001b[38;5;241m.\u001b[39msave_for_backward(t1, t2)\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatrix_multiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/11868/llm_sys_project/minitorch/cuda_kernel_ops.py:358\u001b[0m, in \u001b[0;36mCudaKernelOps.matrix_multiply\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(b\u001b[38;5;241m.\u001b[39m_tensor\u001b[38;5;241m.\u001b[39m_shape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(b\u001b[38;5;241m.\u001b[39m_tensor\u001b[38;5;241m.\u001b[39m_strides) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m--> 358\u001b[0m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMatrixMultiply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strides\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strides\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strides\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# Undo 3d if we added it.\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m both_2d:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmsys",
   "language": "python",
   "name": "llmsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
