{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14b878b9-713c-4f5e-95b4-92ca1360f01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zhenwu/11868/llm_sys_project\n"
     ]
    }
   ],
   "source": [
    "%cd /home/zhenwu/11868/llm_sys_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9c31dec-3f0b-487f-b199-9a40608c53cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import datasets\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import defaultdict\n",
    "# import tqdm\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import random\n",
    "# from bs4 import BeautifulSoup, NavigableString\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Iterator, Callable, Union, Tuple\n",
    "import os\n",
    "import transformers\n",
    "\n",
    "import minitorch\n",
    "from minitorch import DecoderLM\n",
    "from minitorch.tensor import *\n",
    "from minitorch.tensor_functions import *\n",
    "from minitorch.nn import *\n",
    "from minitorch.cuda_kernel_ops import CudaKernelOps\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa356d57-0bcd-4b9a-8001-26bf6d2fdb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imdb(data_path: str = 'data/imdb.json', split: str = None, silent: bool = False, cache_dir: str = None) -> Dict[str, Dict[str, Union[List[Tuple[int, int]], List[str], str]]]:\n",
    "    \"\"\"Load the Anthropic Helpful-Harmless dataset from Huggingface and convert it to the necessary format.\n",
    "       For this dataset, the sft_target is just the chosen response.\n",
    "    \"\"\"\n",
    "    print(f'Loading IMDB RLHF dataset...')\n",
    "    dataset = datasets.load_dataset(\"json\", data_files=\"/home/zhenwu/11868/llm_sys_project/data/imdb.json\")\n",
    "    train_testvalid = dataset['train'].train_test_split(test_size=0.2)\n",
    "\n",
    "    # Split the remaining part into test and validation equally\n",
    "    test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "    \n",
    "    # Create a new dataset dictionary with all three splits\n",
    "    final_dataset = datasets.DatasetDict({\n",
    "        'train': train_testvalid['train'],  # 80% of the data\n",
    "        'test': test_valid['test'],         # 10% of the data\n",
    "        'validation': test_valid['train']   # 10% of the data\n",
    "    })\n",
    "    print('done')\n",
    "\n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b34259bc-fc02-4d3d-8ce0-95edee938423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_batch_element(prompt: str, chosen: str, rejected: str, truncation_mode: str, tokenizer, max_length: int, max_prompt_length: int) -> Dict:\n",
    "    \"\"\"Tokenize a single batch element.\n",
    "    \n",
    "       At this stage, we don't convert to PyTorch tensors yet; we just handle the truncation\n",
    "         in case the prompt + chosen or prompt + rejected responses is/are too long. First\n",
    "         we truncate the prompt; if we're still too long, we truncate the chosen/rejected.\n",
    "       \n",
    "       We also create the labels for the chosen/rejected responses, which are of length equal to\n",
    "         the sum of the length of the prompt and the chosen/rejected response, with -100 for the\n",
    "         prompt tokens.\n",
    "    \"\"\"\n",
    "    chosen_tokens = tokenizer(chosen, add_special_tokens=False)\n",
    "    rejected_tokens = tokenizer(rejected, add_special_tokens=False)\n",
    "    prompt_tokens = tokenizer(prompt, add_special_tokens=False)\n",
    "\n",
    "    # assert tokenizer.eos_token_id not in prompt_tokens['input_ids'], f\"Prompt contains EOS token: {prompt}\"\n",
    "    # assert tokenizer.eos_token_id not in chosen_tokens['input_ids'], f\"Chosen response contains EOS token: {chosen}\"\n",
    "    # assert tokenizer.eos_token_id not in rejected_tokens['input_ids'], f\"Rejected response contains EOS token: {rejected}\"\n",
    "\n",
    "    if tokenizer.eos_token_id not in chosen_tokens['input_ids']:\n",
    "        chosen_tokens['input_ids'].append(tokenizer.eos_token_id)\n",
    "        chosen_tokens['attention_mask'].append(1)\n",
    "    # chosen_tokens['input_ids'].append(tokenizer.eos_token_id)\n",
    "    # chosen_tokens['attention_mask'].append(1)\n",
    "\n",
    "    if tokenizer.eos_token_id not in rejected_tokens['input_ids']:\n",
    "        rejected_tokens['input_ids'].append(tokenizer.eos_token_id)\n",
    "        rejected_tokens['attention_mask'].append(1)\n",
    "\n",
    "    # rejected_tokens['input_ids'].append(tokenizer.eos_token_id)\n",
    "    # rejected_tokens['attention_mask'].append(1)\n",
    "\n",
    "    if len(chosen_tokens['input_ids']) - len(rejected_tokens['input_ids']) > 0:\n",
    "        longer_response_length = len(chosen_tokens['input_ids'])\n",
    "\n",
    "    else:\n",
    "        longer_response_length = len(rejected_tokens['input_ids'])\n",
    "\n",
    "    # longer_response_length = max(len(chosen_tokens['input_ids']), len(rejected_tokens['input_ids']))\n",
    "\n",
    "    # if combined sequence is too long, truncate the prompt\n",
    "    if len(prompt_tokens['input_ids']) + longer_response_length > max_length:\n",
    "        if truncation_mode == 'keep_start':\n",
    "            prompt_tokens = {k: v[:max_prompt_length] for k, v in prompt_tokens.items()}\n",
    "        elif truncation_mode == 'keep_end':\n",
    "            prompt_tokens = {k: v[-max_prompt_length:] for k, v in prompt_tokens.items()}\n",
    "        else:\n",
    "            raise ValueError(f'Unknown truncation mode: {truncation_mode}')\n",
    "\n",
    "    # if that's still too long, truncate the response\n",
    "    if len(prompt_tokens['input_ids']) + longer_response_length > max_length:\n",
    "        chosen_tokens = {k: v[:max_length - max_prompt_length] for k, v in chosen_tokens.items()}\n",
    "        rejected_tokens = {k: v[:max_length - max_prompt_length] for k, v in rejected_tokens.items()}\n",
    "\n",
    "    # Create labels\n",
    "    chosen_sequence_tokens = {k: prompt_tokens[k] + chosen_tokens[k] for k in chosen_tokens}\n",
    "    rejected_sequence_tokens = {k: prompt_tokens[k] + rejected_tokens[k] for k in rejected_tokens}\n",
    "    chosen_sequence_tokens['labels'] = chosen_sequence_tokens['input_ids'][:]\n",
    "    chosen_sequence_tokens['labels'][:len(prompt_tokens['input_ids'])] = [0] * len(prompt_tokens['input_ids'])\n",
    "    rejected_sequence_tokens['labels'] = rejected_sequence_tokens['input_ids'][:]\n",
    "    rejected_sequence_tokens['labels'][:len(prompt_tokens['input_ids'])] = [0] * len(prompt_tokens['input_ids'])\n",
    "\n",
    "    # print(chosen_sequence_tokens['labels'])\n",
    "    \n",
    "    batch = {}\n",
    "\n",
    "    batch['prompt'] = prompt\n",
    "    batch['chosen'] = prompt + chosen\n",
    "    batch['rejected'] = prompt + rejected\n",
    "    batch['chosen_response_only'] = chosen\n",
    "    batch['rejected_response_only'] = rejected\n",
    "\n",
    "    for k, toks in {'chosen': chosen_sequence_tokens, 'rejected': rejected_sequence_tokens, 'prompt': prompt_tokens}.items():\n",
    "        for type_key, tokens in toks.items():\n",
    "            if type_key == 'token_type_ids':\n",
    "                continue\n",
    "            batch[f'{k}_{type_key}'] = tokens\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5073aac1-5f03-462d-8760-1c1cf3220296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(\n",
    "        examples, tokenizer, model_max_length, backend=None):\n",
    "    \"\"\"\n",
    "    Prepares a batch of examples for model training or evaluation by tokenizing and padding them.\n",
    "\n",
    "    Parameters:\n",
    "    - examples: A list of examples to be processed.\n",
    "    - src_key: The key for accessing source texts in the examples.\n",
    "    - tgt_key: The key for accessing target texts in the examples.\n",
    "    - tokenizer: The tokenizer to be used for encoding the texts.\n",
    "    - model_max_length: The maximum sequence length the model can handle.\n",
    "    - backend: The backend of minitorch tensors.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing keys: 'input_ids', 'labels', 'label_token_weights',\n",
    "        each indicates a minitorch tensor with shape (len(examples), model_max_length).\n",
    "\n",
    "    Notes:\n",
    "    [\"input_ids\"] for every example in the DE-EN translation, the \"input_ids\" will be:\n",
    "        <de_token_ids> + <de_eos_id> + <en_token_ids> + <en_eos_id> + <pad_ids>\n",
    "    where the pad_ids makes the length of input_ids to be model_max_length.\n",
    "\n",
    "    [\"labels\"]: the next tokens to be predicted, which will be used in the cross-entropy\n",
    "    loss function, e.g., for an example tokenized as [a, b, c, d], \"input_ids\" and \"labels\" \n",
    "    can be [a, b, c] and [b, c, d], respectively.\n",
    "\n",
    "    [\"label_token_weights\"] The 'label_token_weights' are used to differentiate\n",
    "    calculation purposes. (the MLE loss is computed on target tokens only.)\n",
    "    between the source (weight = 0) and target (weight = 1) tokens for loss\n",
    "\n",
    "    TODO: \n",
    "        outputs: [chosen token ids, prompt token ids, rejected token ids]\n",
    "    \n",
    "    \"\"\"\n",
    "    token_ids_chosen, token_masks_chosen, token_ids_rejected, token_masks_rejected = [], [], [], []\n",
    "    label_ids_chosen, label_ids_rejected = [], []\n",
    "    # pad_token_id = tokenizer.vocab['<pad>']\n",
    "    pad_token_id = tokenizer.encode('<pad>')[1]\n",
    "    # print(pad_token_id)\n",
    "    for i in range(len(examples['prompt'])):\n",
    "        # print('example: ', example)\n",
    "        batch = tokenize_batch_element(examples['prompt'][i], examples['chosen'][i], examples['rejected'][i], 'keep_start', tokenizer, 512, 256)\n",
    "\n",
    "        # ------------ToDo------------\n",
    "        # token_ids_chosen = tokenizer(\n",
    "        #    f'{example[chosen_key]}<eos_{chosen_key}>')['input_ids']\n",
    "        # ------------ToDo------------\n",
    "\n",
    "        # BEGIN ASSIGN2_2\n",
    "        # TODO\n",
    "        # create token_ids, labels, and label_token_weights for every example\n",
    "        # hint: based on token_ids_src, token_ids_tgt, and pad_token_id\n",
    "        \n",
    "        # input_ids is <de_token_ids> + <de_eos_id> + <en_token_ids> + <en_eos_id> + <pad_ids> where \n",
    "        # the pad_ids makes the length of input_ids to be model_max_length\n",
    "\n",
    "        # total_len = len(token_ids_src) + len(token_ids_tgt)\n",
    "        # token_pad_length = model_max_length - total_len\n",
    "\n",
    "        total_len_chosen = len(batch['chosen_input_ids'])\n",
    "        token_pad_length_chosen = model_max_length - total_len_chosen\n",
    "        \n",
    "        \n",
    "        total_len_rejected = len(batch['rejected_input_ids'])\n",
    "        token_pad_length_rejected = model_max_length - total_len_rejected\n",
    "        \n",
    "        # pad \n",
    "        token_id_chosen = batch['chosen_input_ids'] + [pad_token_id] * token_pad_length_chosen\n",
    "        # print('token_id_chosen: ', len(batch['chosen_input_ids']))\n",
    "        token_mask_chosen = [1] * len(batch['chosen_input_ids']) + [0] * token_pad_length_chosen\n",
    "        label_id_chosen = batch['chosen_labels'] + [0] * token_pad_length_chosen\n",
    "        token_id_chosen = token_id_chosen[:model_max_length]\n",
    "        token_mask_chosen = token_mask_chosen[:model_max_length]\n",
    "\n",
    "        token_id_chosen = token_id_chosen[:-1]\n",
    "        token_mask_chosen = token_mask_chosen[1:]\n",
    "        label_id_chosen = label_id_chosen[1:]\n",
    "        \n",
    "        token_ids_chosen.append(token_id_chosen)\n",
    "        token_masks_chosen.append(token_mask_chosen)\n",
    "        label_ids_chosen.append(label_id_chosen)\n",
    "        \n",
    "        token_id_rejected = batch['rejected_input_ids'] + [pad_token_id] * token_pad_length_rejected\n",
    "        token_mask_rejected = [1] * len(batch['rejected_input_ids']) + [0] * token_pad_length_rejected\n",
    "        label_id_rejected = batch['rejected_labels'] + [0] * token_pad_length_rejected\n",
    "        token_id_rejected = token_id_rejected[:model_max_length]\n",
    "        token_mask_rejected = token_mask_rejected[:model_max_length]\n",
    "\n",
    "        token_id_rejected = token_id_rejected[:-1]\n",
    "        token_mask_rejected = token_mask_rejected[1:]\n",
    "        label_id_rejected = label_id_rejected[1:]\n",
    "        \n",
    "        token_ids_rejected.append(token_id_rejected)\n",
    "        token_masks_rejected.append(token_mask_rejected)\n",
    "        label_ids_rejected.append(label_id_rejected)\n",
    "        \n",
    "        # assert len(token_id_chosen) == model_max_length, print(len(token_id_chosen), total_len_chosen, token_pad_length_chosen)\n",
    "    \n",
    "\n",
    "    return {\n",
    "        'chosen_input_ids': minitorch.tensor_functions.tensor_from_numpy(np.array(token_ids_chosen), backend),\n",
    "        # 'chosen_masks': minitorch.tensor_functions.tensor_from_numpy(np.array(token_masks_chosen), backend),\n",
    "        # 'chosen_labels': minitorch.tensor_functions.tensor_from_numpy(np.array(label_ids_chosen), backend),\n",
    "        # 'rejected_input_ids': minitorch.tensor_functions.tensor_from_numpy(np.array(token_ids_rejected), backend),\n",
    "        # 'rejected_masks': minitorch.tensor_functions.tensor_from_numpy(np.array(token_masks_rejected), backend),\n",
    "        # 'rejected_labels': minitorch.tensor_functions.tensor_from_numpy(np.array(label_ids_rejected), backend),\n",
    "        'concatenated_input_ids': minitorch.tensor_functions.tensor_from_numpy(np.concatenate((np.array(token_ids_chosen), np.array(token_ids_rejected))), backend),\n",
    "        'concatenated_masks': minitorch.tensor_functions.tensor_from_numpy(np.concatenate((np.array(token_masks_chosen), np.array(token_masks_rejected))), backend),\n",
    "        'concatenated_labels': minitorch.tensor_functions.tensor_from_numpy(np.concatenate((np.array(label_ids_chosen), np.array(label_ids_rejected))), backend)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0791bdf-23de-4eab-8e19-1772c341c1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather(input, dim, index):\n",
    "    batch_size, seq_len = index.shape\n",
    "    mask = index.zeros(shape=input.shape)  # Assuming Minitorch has a method to create a zero tensor\n",
    "\n",
    "    # We use nested loops to iterate over each dimension except the one we are gathering from\n",
    "    if dim == 2:  # As per your use case, gathering from the last dimension\n",
    "        for i in range(input.shape[0]):  # Loop over the first dimension (batch size)\n",
    "            for j in range(input.shape[1]):  # Loop over the second dimension (sequence length)\n",
    "                k = int(index[i, j])  # This is the index in the vocab size dimension\n",
    "                # print(mask.shape)\n",
    "                mask[i, j, k] = 1\n",
    "    else:\n",
    "        raise NotImplementedError(\"Gather function only implemented for dim=2\")\n",
    "\n",
    "    output = input * mask\n",
    "    output = output.sum(dim=dim).view(batch_size, seq_len)\n",
    "    # print(output.shape)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2d0c7dc-8a7d-4f18-a8d0-fca5a8787ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backend = minitorch.TensorBackend(CudaKernelOps)\n",
    "\n",
    "# np.random.seed(0)\n",
    "\n",
    "# logits = tensor_from_numpy(np.random.rand(2, 3, 4), backend=backend)\n",
    "# labels = tensor_from_numpy(np.ones((2, 3)), backend=backend)\n",
    "\n",
    "# output = gather(logits, 2, labels)\n",
    "\n",
    "# logps = tensor_from_numpy(np.random.rand(20, 1), backend=backend)\n",
    "\n",
    "# print(logps)\n",
    "\n",
    "# # batch = tensor_from_numpy(np.ones((20, 1)), backend=backend)\n",
    "\n",
    "# c, r = split_tensor(logps, 10)\n",
    "\n",
    "# print(c)\n",
    "# print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8590a28d-f370-46aa-8cd7-869dcf12f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tensor(tensor, split_index):\n",
    "    # Assuming tensor has shape [N] and split_index is the point to split\n",
    "    part1 = tensor.zeros(tensor.shape)\n",
    "    part2 = tensor.ones(tensor.shape)\n",
    "\n",
    "    for i in range(split_index):\n",
    "        part1[i, 0] = 1\n",
    "        part2[i, 0] = 0\n",
    "\n",
    "    chosen = tensor * part1\n",
    "    rejected = tensor * part2\n",
    "\n",
    "    return chosen, rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adfce2aa-2a70-4244-ba4c-92817606cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(batch, model, backend):\n",
    "    \"\"\"\n",
    "    The MLE loss for a batch.\n",
    "\n",
    "    Parameters:\n",
    "    - batch: The result of collate_fn, a dict with \"input_ids\", \"labels\", and \"label_token_weights\".\n",
    "    - model: The model to be trained.\n",
    "\n",
    "    Returns:\n",
    "    - A scalar loss value for this batch, averaged across all target tokens.\n",
    "\n",
    "    # ------------ToDo------------\n",
    "    add preference loss\n",
    "    add preference model\n",
    "    # ------------ToDo------------\n",
    "    \"\"\"\n",
    "\n",
    "    idx = batch['concatenated_input_ids']\n",
    "    idx.requires_grad_(True)\n",
    "    \n",
    "    logits = model(idx=idx)\n",
    "    \n",
    "    labels = batch['concatenated_labels']\n",
    "\n",
    "    loss_mask = batch['concatenated_masks']\n",
    "  \n",
    "    batch_size, seq_len, vocab_size = logits.shape\n",
    "\n",
    "    per_token_logps = gather(logsoftmax(logits, -1), 2, labels)\n",
    "\n",
    "    # print('loss_mask', loss_mask)\n",
    "\n",
    "    # print('per token logps', per_token_logps)\n",
    "\n",
    "    batch_logps = (per_token_logps * loss_mask).sum(1)\n",
    "\n",
    "    # print('batch_logps shape: ', batch_logps.shape)\n",
    "\n",
    "    chosen_length = batch_logps.shape[0] // 2\n",
    "\n",
    "    chosen_logps, rejected_logps = split_tensor(batch_logps, chosen_length)\n",
    "\n",
    "    # print('chosen_logps: ', chosen_logps)\n",
    "    # print('rejected_logps: ', rejected_logps)\n",
    "    \n",
    "    loss = minitorch.nn.preference_loss(chosen_logps, rejected_logps, beta=0.7)\n",
    "\n",
    "    # loss = minitorch.nn.softmax_loss(logits.view(batch_size * seq_len, vocab_size), labels.view(batch_size * seq_len,))\n",
    "    # loss = loss.view(batch_size, seq_len)\n",
    "    \n",
    "    batch_loss = loss.mean()\n",
    "\n",
    "    print('batch_loss: ', batch_loss)\n",
    "\n",
    "    # print(\"Gradients tracking on logits:\", logits.requires_grad())\n",
    "    # print(\"Gradients tracking on chosen_logps:\", chosen_logps.requires_grad())\n",
    "    # print(\"Gradients tracking on rejected_logps:\", rejected_logps.requires_grad())\n",
    "    # print(\"Gradients tracking on batch loss:\", batch_loss.requires_grad())\n",
    "    # print(\"Gradients tracking on batch loss:\", batch_loss.requires_grad())\n",
    "\n",
    "    # return batch_loss, chosen_rewards, rejected_rewards\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6741bc4-3391-48b9-959f-df904c644b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, examples, n_samples, collate_fn, batch_size, desc, backend):\n",
    "    \"\"\"\n",
    "    Trains the model on the provided examples.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The model to be trained.\n",
    "    - optimizer: The optimizer used for updating the model's parameters.\n",
    "    - examples: The dataset examples used for training.\n",
    "    - n_samples: The random samples to train from \"examples\".\n",
    "    - collate_fn: The function to collate data examples into batches.\n",
    "    - batch_size: The number of examples in each batch.\n",
    "    - desc: Description for the training process (used in progress bars).\n",
    "\n",
    "    # ------------ToDo------------\n",
    "    add preference policy model\n",
    "    # ------------ToDo------------\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    # random.shuffle(examples)\n",
    "    # examples = examples[:n_samples]\n",
    "\n",
    "    for i in range(0, len(examples), batch_size):\n",
    "        batch = collate_fn(examples=examples[i:i + batch_size])\n",
    "\n",
    "        t0 = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(batch=batch, model=model, backend=backend)\n",
    "        t1 = time.time()\n",
    "\n",
    "        loss.backward()\n",
    "        # print(loss.grad)\n",
    "        t2 = time.time()\n",
    "\n",
    "        optimizer.step()\n",
    "        t3 = time.time()\n",
    "\n",
    "        # print(f\"Forward: {t1 - t0}\")\n",
    "        # print(f\"Backward: {t2 - t1}\")\n",
    "        # print(f\"Opt.step: {t3 - t2}\")\n",
    "\n",
    "        batch_time = time.time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b44b4c7b-3d86-4a45-ac26-659cf8fe965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(model, examples, batch_size, collate_fn, desc, backend):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the provided examples and computes the average loss.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The model to be evaluated.\n",
    "    - examples: The dataset examples used for evaluation.\n",
    "    - batch_size: The number of examples in each batch.\n",
    "    - collate_fn: The function to collate data examples into batches.\n",
    "    - desc: Description for the evaluation process (used in progress bars).\n",
    "\n",
    "    Returns:\n",
    "    - The average loss computed over all batches.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "\n",
    "    for i in range(len(examples)):\n",
    "        batch = collate_fn(examples=examples[i:i + batch_size])\n",
    "        loss = loss_fn(batch=batch, model=model, backend=backend)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        # prog_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aed3dc0e-316c-4977-945c-a58217f11819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: modify generate for our need\n",
    "\n",
    "def generate(model,\n",
    "             examples,\n",
    "             tokenizer,\n",
    "             model_max_length,\n",
    "             backend,\n",
    "             desc):\n",
    "    \"\"\"\n",
    "    Generates target sequences for the given source sequences using the model, based on argmax decoding.\n",
    "    Note that it runs generation on examples one-by-one instead of in a batched manner.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The model used for generation.\n",
    "    - examples: The dataset examples containing source sequences.\n",
    "    - tokenizer: The tokenizer used for encoding texts.\n",
    "    - model_max_length: The maximum sequence length the model can handle.\n",
    "    - backend: The backend of minitorch tensors.\n",
    "    - desc: Description for the generation process (used in progress bars).\n",
    "\n",
    "    Returns:\n",
    "    - A list of generated target sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    gen_sents = []\n",
    "    # for example in tqdm.tqdm(examples, desc=f'Generating {desc}'):\n",
    "    for example in examples:\n",
    "        # Run generation for every single example\n",
    "\n",
    "        token_ids = tokenizer(f'{example[src_key]}<eos_{src_key}>')['input_ids']\n",
    "        len_src = len(token_ids)\n",
    "\n",
    "        while len(token_ids) <= model_max_length:\n",
    "            # BEGIN ASSIGN2_2\n",
    "            # TODO\n",
    "            # run the model with current token_ids, and predict the next token (gen_id)\n",
    "            # hint: obtain the logits of next token, and take the argmax.\n",
    "\n",
    "            token_ids_tensor = tensor_from_numpy(np.array([token_ids]), backend)\n",
    "            \n",
    "            # get logits\n",
    "            logits = model(idx=token_ids_tensor)\n",
    "            # logits of the last token\n",
    "            logits_np = logits.to_numpy()[:, -1, :]\n",
    "\n",
    "            # get the argmax\n",
    "            gen_id = np.argmax(logits_np, axis=-1).item()\n",
    "\n",
    "            # END ASSIGN2_2\n",
    "\n",
    "            if gen_id == tokenizer.vocab[f'<eos_{tgt_key}>']:\n",
    "                break\n",
    "            else:\n",
    "                token_ids.append(gen_id)\n",
    "\n",
    "        gen_sents.append(tokenizer.decode(token_ids[len_src:]))\n",
    "\n",
    "    return gen_sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beadd9d-d138-4d6a-8031-8e6778f67870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add eval metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe3f925e-b555-4be3-9c9f-2f35ddbb5f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model_max_length=512,\n",
    "         n_epochs=5,\n",
    "         batch_size=10,\n",
    "         learning_rate=0.02,\n",
    "         samples_per_epoch=2,\n",
    "         n_vocab=10000,\n",
    "         n_embd=256,\n",
    "         seed=11111):\n",
    "    \"\"\"\n",
    "    The main function to train and evaluate the model on a specified dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_name: The name of the dataset to be used.\n",
    "    - model_max_length: The maximum sequence length the model can handle.\n",
    "    - n_epochs: The number of training epochs.\n",
    "    - batch_size: The number of examples in each batch.\n",
    "    - learning_rate: The learning rate for the optimizer.\n",
    "    - samples_per_epoch: Samples from the training dataset every epoch.\n",
    "    - n_vocab: The vocabulary size of the BPE tokenizer.\n",
    "    - n_embd: The embedding dimension.\n",
    "    - seed: Random seed.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # workdir = f'./workdir_vocab{n_vocab}_lr{learning_rate}_embd{n_embd}'\n",
    "    # os.makedirs(workdir, exist_ok=True)\n",
    "\n",
    "    backend = minitorch.TensorBackend(CudaKernelOps)\n",
    "\n",
    "    config = {\n",
    "        'n_vocab': 50257,  # vocab_size\n",
    "        'n_embd': 256,  # n_embed\n",
    "        'n_head': 8,  # n_head\n",
    "        'n_positions': 512,  # n_ctx == n_positions\n",
    "        # 'n_layer'     : 4,    # n_layer\n",
    "        'p_dropout': 0.1,  # x_pdrop\n",
    "        'ln_eps': 1e-5,  # layer_norm_epsilon\n",
    "        'backend': backend\n",
    "    }\n",
    "\n",
    "    model = DecoderLM(**config)\n",
    "    optimizer = minitorch.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    dataset = get_imdb()\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2-large', special_tokens={'pad_token': '<pad>'})\n",
    "\n",
    "    collate_fn = partial(collate_batch, tokenizer=tokenizer, model_max_length=512, backend=backend)\n",
    "\n",
    "    for epoch_idx in range(n_epochs):\n",
    "        desc = f'epoch {epoch_idx} / {n_epochs}'\n",
    "\n",
    "        train(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            examples=dataset['train'],\n",
    "            n_samples=samples_per_epoch,\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=collate_fn,\n",
    "            desc=desc,\n",
    "            backend=backend)\n",
    "\n",
    "        validation_loss = evaluate_loss(\n",
    "            model=model,\n",
    "            examples=dataset['validation'],\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=collate_fn,\n",
    "            desc=desc)\n",
    "\n",
    "        gen_sents = generate(\n",
    "            model=model,\n",
    "            examples=dataset['test'],\n",
    "            tokenizer=tokenizer,\n",
    "            model_max_length=model_max_length,\n",
    "            backend=backend,\n",
    "            desc=desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d0de8c5-c978-439b-b28f-93c94cd0eb6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB RLHF dataset...\n",
      "done\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 48\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(model_max_length, n_epochs, batch_size, learning_rate, samples_per_epoch, n_vocab, n_embd, seed)\u001b[0m\n\u001b[1;32m     46\u001b[0m dataset \u001b[38;5;241m=\u001b[39m get_imdb()\n\u001b[1;32m     47\u001b[0m a \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 48\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     49\u001b[0m c \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     50\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mAutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2-large\u001b[39m\u001b[38;5;124m'\u001b[39m, special_tokens\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad_token\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "File \u001b[0;32m~/anaconda3/envs/llmsys/lib/python3.9/site-packages/datasets/dataset_dict.py:74\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 74\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     77\u001b[0m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     78\u001b[0m         ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'validation'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmsys",
   "language": "python",
   "name": "llmsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
