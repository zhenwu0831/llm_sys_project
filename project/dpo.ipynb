{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9c31dec-3f0b-487f-b199-9a40608c53cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import datasets\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import defaultdict\n",
    "# import tqdm\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import random\n",
    "# from bs4 import BeautifulSoup, NavigableString\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Iterator, Callable, Union, Tuple\n",
    "import os\n",
    "import transformers\n",
    "\n",
    "import minitorch\n",
    "from minitorch import DecoderLM\n",
    "from minitorch.tensor import *\n",
    "from minitorch.tensor_functions import *\n",
    "from minitorch.nn import *\n",
    "from minitorch.cuda_kernel_ops import CudaKernelOps\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14b878b9-713c-4f5e-95b4-92ca1360f01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zhenwu/11868/llm_sys_project\n"
     ]
    }
   ],
   "source": [
    "%cd /home/zhenwu/11868/llm_sys_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa356d57-0bcd-4b9a-8001-26bf6d2fdb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imdb(data_path: str = 'data/imdb.json', split: str = None, silent: bool = False, cache_dir: str = None) -> Dict[str, Dict[str, Union[List[Tuple[int, int]], List[str], str]]]:\n",
    "    \"\"\"Load the Anthropic Helpful-Harmless dataset from Huggingface and convert it to the necessary format.\n",
    "       For this dataset, the sft_target is just the chosen response.\n",
    "    \"\"\"\n",
    "    print(f'Loading IMDB RLHF dataset...')\n",
    "    dataset = datasets.load_dataset(\"json\", data_files=\"/home/zhenwu/11868/llm_sys_project/data/imdb.json\")\n",
    "    print('done')\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b34259bc-fc02-4d3d-8ce0-95edee938423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_batch_element(prompt: str, chosen: str, rejected: str, truncation_mode: str, tokenizer, max_length: int, max_prompt_length: int) -> Dict:\n",
    "    \"\"\"Tokenize a single batch element.\n",
    "    \n",
    "       At this stage, we don't convert to PyTorch tensors yet; we just handle the truncation\n",
    "         in case the prompt + chosen or prompt + rejected responses is/are too long. First\n",
    "         we truncate the prompt; if we're still too long, we truncate the chosen/rejected.\n",
    "       \n",
    "       We also create the labels for the chosen/rejected responses, which are of length equal to\n",
    "         the sum of the length of the prompt and the chosen/rejected response, with -100 for the\n",
    "         prompt tokens.\n",
    "    \"\"\"\n",
    "    chosen_tokens = tokenizer(chosen, add_special_tokens=False)\n",
    "    rejected_tokens = tokenizer(rejected, add_special_tokens=False)\n",
    "    prompt_tokens = tokenizer(prompt, add_special_tokens=False)\n",
    "\n",
    "    # assert tokenizer.eos_token_id not in prompt_tokens['input_ids'], f\"Prompt contains EOS token: {prompt}\"\n",
    "    # assert tokenizer.eos_token_id not in chosen_tokens['input_ids'], f\"Chosen response contains EOS token: {chosen}\"\n",
    "    # assert tokenizer.eos_token_id not in rejected_tokens['input_ids'], f\"Rejected response contains EOS token: {rejected}\"\n",
    "\n",
    "    if tokenizer.eos_token_id not in chosen_tokens['input_ids']:\n",
    "        chosen_tokens['input_ids'].append(tokenizer.eos_token_id)\n",
    "        chosen_tokens['attention_mask'].append(1)\n",
    "    # chosen_tokens['input_ids'].append(tokenizer.eos_token_id)\n",
    "    # chosen_tokens['attention_mask'].append(1)\n",
    "\n",
    "    if tokenizer.eos_token_id not in rejected_tokens['input_ids']:\n",
    "        rejected_tokens['input_ids'].append(tokenizer.eos_token_id)\n",
    "        rejected_tokens['attention_mask'].append(1)\n",
    "\n",
    "    # rejected_tokens['input_ids'].append(tokenizer.eos_token_id)\n",
    "    # rejected_tokens['attention_mask'].append(1)\n",
    "\n",
    "    if len(chosen_tokens['input_ids']) - len(rejected_tokens['input_ids']) > 0:\n",
    "        longer_response_length = len(chosen_tokens['input_ids'])\n",
    "\n",
    "    else:\n",
    "        longer_response_length = len(rejected_tokens['input_ids'])\n",
    "\n",
    "    # longer_response_length = max(len(chosen_tokens['input_ids']), len(rejected_tokens['input_ids']))\n",
    "\n",
    "    # if combined sequence is too long, truncate the prompt\n",
    "    if len(prompt_tokens['input_ids']) + longer_response_length > max_length:\n",
    "        if truncation_mode == 'keep_start':\n",
    "            prompt_tokens = {k: v[:max_prompt_length] for k, v in prompt_tokens.items()}\n",
    "        elif truncation_mode == 'keep_end':\n",
    "            prompt_tokens = {k: v[-max_prompt_length:] for k, v in prompt_tokens.items()}\n",
    "        else:\n",
    "            raise ValueError(f'Unknown truncation mode: {truncation_mode}')\n",
    "\n",
    "    # if that's still too long, truncate the response\n",
    "    if len(prompt_tokens['input_ids']) + longer_response_length > max_length:\n",
    "        chosen_tokens = {k: v[:max_length - max_prompt_length] for k, v in chosen_tokens.items()}\n",
    "        rejected_tokens = {k: v[:max_length - max_prompt_length] for k, v in rejected_tokens.items()}\n",
    "\n",
    "    # Create labels\n",
    "    chosen_sequence_tokens = {k: prompt_tokens[k] + chosen_tokens[k] for k in chosen_tokens}\n",
    "    rejected_sequence_tokens = {k: prompt_tokens[k] + rejected_tokens[k] for k in rejected_tokens}\n",
    "    chosen_sequence_tokens['labels'] = chosen_sequence_tokens['input_ids'][:]\n",
    "    chosen_sequence_tokens['labels'][:len(prompt_tokens['input_ids'])] = [0] * len(prompt_tokens['input_ids'])\n",
    "    rejected_sequence_tokens['labels'] = rejected_sequence_tokens['input_ids'][:]\n",
    "    rejected_sequence_tokens['labels'][:len(prompt_tokens['input_ids'])] = [0] * len(prompt_tokens['input_ids'])\n",
    "\n",
    "    # print(chosen_sequence_tokens['labels'])\n",
    "    \n",
    "    batch = {}\n",
    "\n",
    "    batch['prompt'] = prompt\n",
    "    batch['chosen'] = prompt + chosen\n",
    "    batch['rejected'] = prompt + rejected\n",
    "    batch['chosen_response_only'] = chosen\n",
    "    batch['rejected_response_only'] = rejected\n",
    "\n",
    "    for k, toks in {'chosen': chosen_sequence_tokens, 'rejected': rejected_sequence_tokens, 'prompt': prompt_tokens}.items():\n",
    "        for type_key, tokens in toks.items():\n",
    "            if type_key == 'token_type_ids':\n",
    "                continue\n",
    "            batch[f'{k}_{type_key}'] = tokens\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5073aac1-5f03-462d-8760-1c1cf3220296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(\n",
    "        examples, tokenizer, model_max_length, backend=None):\n",
    "    \"\"\"\n",
    "    Prepares a batch of examples for model training or evaluation by tokenizing and padding them.\n",
    "\n",
    "    Parameters:\n",
    "    - examples: A list of examples to be processed.\n",
    "    - src_key: The key for accessing source texts in the examples.\n",
    "    - tgt_key: The key for accessing target texts in the examples.\n",
    "    - tokenizer: The tokenizer to be used for encoding the texts.\n",
    "    - model_max_length: The maximum sequence length the model can handle.\n",
    "    - backend: The backend of minitorch tensors.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing keys: 'input_ids', 'labels', 'label_token_weights',\n",
    "        each indicates a minitorch tensor with shape (len(examples), model_max_length).\n",
    "\n",
    "    Notes:\n",
    "    [\"input_ids\"] for every example in the DE-EN translation, the \"input_ids\" will be:\n",
    "        <de_token_ids> + <de_eos_id> + <en_token_ids> + <en_eos_id> + <pad_ids>\n",
    "    where the pad_ids makes the length of input_ids to be model_max_length.\n",
    "\n",
    "    [\"labels\"]: the next tokens to be predicted, which will be used in the cross-entropy\n",
    "    loss function, e.g., for an example tokenized as [a, b, c, d], \"input_ids\" and \"labels\" \n",
    "    can be [a, b, c] and [b, c, d], respectively.\n",
    "\n",
    "    [\"label_token_weights\"] The 'label_token_weights' are used to differentiate\n",
    "    calculation purposes. (the MLE loss is computed on target tokens only.)\n",
    "    between the source (weight = 0) and target (weight = 1) tokens for loss\n",
    "\n",
    "    TODO: \n",
    "        outputs: [chosen token ids, prompt token ids, rejected token ids]\n",
    "    \n",
    "    \"\"\"\n",
    "    token_ids_chosen, token_masks_chosen, token_ids_rejected, token_masks_rejected = [], [], [], []\n",
    "    label_ids_chosen, label_ids_rejected = [], []\n",
    "    # pad_token_id = tokenizer.vocab['<pad>']\n",
    "    pad_token_id = tokenizer.encode('<pad>')[1]\n",
    "    # print(pad_token_id)\n",
    "    for i in range(len(examples['prompt'])):\n",
    "        # print('example: ', example)\n",
    "        batch = tokenize_batch_element(examples['prompt'][i], examples['chosen'][i], examples['rejected'][i], 'keep_start', tokenizer, 512, 256)\n",
    "\n",
    "        # ------------ToDo------------\n",
    "        # token_ids_chosen = tokenizer(\n",
    "        #    f'{example[chosen_key]}<eos_{chosen_key}>')['input_ids']\n",
    "        # ------------ToDo------------\n",
    "\n",
    "        # BEGIN ASSIGN2_2\n",
    "        # TODO\n",
    "        # create token_ids, labels, and label_token_weights for every example\n",
    "        # hint: based on token_ids_src, token_ids_tgt, and pad_token_id\n",
    "        \n",
    "        # input_ids is <de_token_ids> + <de_eos_id> + <en_token_ids> + <en_eos_id> + <pad_ids> where \n",
    "        # the pad_ids makes the length of input_ids to be model_max_length\n",
    "\n",
    "        # total_len = len(token_ids_src) + len(token_ids_tgt)\n",
    "        # token_pad_length = model_max_length - total_len\n",
    "\n",
    "        total_len_chosen = len(batch['chosen_input_ids'])\n",
    "        token_pad_length_chosen = model_max_length - total_len_chosen\n",
    "        \n",
    "        \n",
    "        \n",
    "        total_len_rejected = len(batch['rejected_input_ids'])\n",
    "        token_pad_length_rejected = model_max_length - total_len_rejected\n",
    "        \n",
    "        # pad \n",
    "        token_id_chosen = batch['chosen_input_ids'] + [pad_token_id] * token_pad_length_chosen\n",
    "        # print('token_id_chosen: ', len(batch['chosen_input_ids']))\n",
    "        token_mask_chosen = [1] * len(batch['chosen_input_ids']) + [0] * token_pad_length_chosen\n",
    "        label_id_chosen = batch['chosen_labels'] + [-0] * token_pad_length_chosen\n",
    "        token_id_chosen = token_id_chosen[:model_max_length]\n",
    "        token_mask_chosen = token_mask_chosen[:model_max_length]\n",
    "        \n",
    "        token_ids_chosen.append(token_id_chosen)\n",
    "        token_masks_chosen.append(token_mask_chosen)\n",
    "        label_ids_chosen.append(label_id_chosen)\n",
    "        \n",
    "        token_id_rejected = batch['rejected_input_ids'] + [pad_token_id] * token_pad_length_rejected\n",
    "        token_mask_rejected = [1] * len(batch['rejected_input_ids']) + [0] * token_pad_length_rejected\n",
    "        label_id_rejected = batch['rejected_labels'] + [0] * token_pad_length_rejected\n",
    "        token_id_rejected = token_id_rejected[:model_max_length]\n",
    "        token_mask_rejected = token_mask_rejected[:model_max_length]\n",
    "        \n",
    "        token_ids_rejected.append(token_id_rejected)\n",
    "        token_masks_rejected.append(token_mask_rejected)\n",
    "        label_ids_rejected.append(label_id_rejected)\n",
    "        \n",
    "        assert len(token_id_chosen) == model_max_length, print(len(token_id_chosen), total_len_chosen, token_pad_length_chosen)\n",
    "    \n",
    "\n",
    "    return {\n",
    "        'chosen_input_ids': minitorch.tensor_functions.tensor_from_numpy(np.array(token_ids_chosen), backend),\n",
    "        # 'chosen_masks': minitorch.tensor_functions.tensor_from_numpy(np.array(token_masks_chosen), backend),\n",
    "        # 'chosen_labels': minitorch.tensor_functions.tensor_from_numpy(np.array(label_ids_chosen), backend),\n",
    "        # 'rejected_input_ids': minitorch.tensor_functions.tensor_from_numpy(np.array(token_ids_rejected), backend),\n",
    "        # 'rejected_masks': minitorch.tensor_functions.tensor_from_numpy(np.array(token_masks_rejected), backend),\n",
    "        # 'rejected_labels': minitorch.tensor_functions.tensor_from_numpy(np.array(label_ids_rejected), backend),\n",
    "        'concatenated_input_ids': minitorch.tensor_functions.tensor_from_numpy(np.concatenate((np.array(token_ids_chosen), np.array(token_ids_rejected))), backend),\n",
    "        'concatenated_masks': minitorch.tensor_functions.tensor_from_numpy(np.concatenate((np.array(token_masks_chosen), np.array(token_masks_rejected))), backend),\n",
    "        'concatenated_labels': minitorch.tensor_functions.tensor_from_numpy(np.concatenate((np.array(label_ids_chosen), np.array(label_ids_rejected))), backend)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b0791bdf-23de-4eab-8e19-1772c341c1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_gather(data, indices):\n",
    "    \"\"\"Custom implementation of gather operation.\"\"\"\n",
    "    data \n",
    "\n",
    "\n",
    "def simulate_torch_operations(logits, labels):\n",
    "    # Assuming logits is a list of lists and labels is a list of indices\n",
    "    # Convert logits to log softmax\n",
    "    # log_softmax_logits = [logsoftmax(logit, 1) for logit in logits]\n",
    "    log_softmax_logits = logsoftmax(logits, 2)\n",
    "\n",
    "    for i in range(log_softmax_logits.shape[0]):\n",
    "        log_softmax_logit = log_softmax_logits[i]\n",
    "        label = labels[i]\n",
    "        gathered = custom_gather(log_softmax_logit, label)\n",
    "\n",
    "    # Gather using the custom gather function\n",
    "    # labels.unsqueeze(2) equivalent in basic python could be transforming label indices\n",
    "    zipped = zip(log_softmax_logits, labels)\n",
    "    for x, y in zipped:\n",
    "        print(x, y)\n",
    "    gathered_results = [custom_gather(log_softmax_logit, label.view(log_softmax_logit).shape) for log_softmax_logit, label in zip(log_softmax_logits, labels)]\n",
    "\n",
    "    # Squeeze operation by selecting the first element since we know it's a single element list\n",
    "    squeezed_results = [result[0] for result in gathered_results]\n",
    "    \n",
    "    return squeezed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "802cccad-e90d-4ee5-8bbe-d1603c582306",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexingError",
     "evalue": "Index [0] must be size of (20, 30, 40).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m logits \u001b[38;5;241m=\u001b[39m a\n\u001b[1;32m      3\u001b[0m labels \u001b[38;5;241m=\u001b[39m a\n\u001b[0;32m----> 4\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[43msimulate_torch_operations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[72], line 15\u001b[0m, in \u001b[0;36msimulate_torch_operations\u001b[0;34m(logits, labels)\u001b[0m\n\u001b[1;32m     12\u001b[0m log_softmax_logits \u001b[38;5;241m=\u001b[39m logsoftmax(logits, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(log_softmax_logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m---> 15\u001b[0m     log_softmax_logit \u001b[38;5;241m=\u001b[39m \u001b[43mlog_softmax_logits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     16\u001b[0m     label \u001b[38;5;241m=\u001b[39m labels[i]\n\u001b[1;32m     17\u001b[0m     gathered \u001b[38;5;241m=\u001b[39m custom_gather(log_softmax_logit, label)\n",
      "File \u001b[0;32m~/11868/llm_sys_project/minitorch/tensor.py:282\u001b[0m, in \u001b[0;36mTensor.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: Union[\u001b[38;5;28mint\u001b[39m, UserIndex]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    281\u001b[0m     key2 \u001b[38;5;241m=\u001b[39m (key,) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m key\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/11868/llm_sys_project/minitorch/tensor_data.py:240\u001b[0m, in \u001b[0;36mTensorData.get\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: UserIndex) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m     x: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/11868/llm_sys_project/minitorch/tensor_data.py:219\u001b[0m, in \u001b[0;36mTensorData.index\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# Check for errors\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aindex\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IndexingError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be size of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, ind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(aindex):\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ind \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[i]:\n",
      "\u001b[0;31mIndexingError\u001b[0m: Index [0] must be size of (20, 30, 40)."
     ]
    }
   ],
   "source": [
    "a = tensor_from_numpy(np.ones((20, 30, 40)), backend = minitorch.TensorBackend(CudaKernelOps))\n",
    "logits = a\n",
    "labels = a\n",
    "b = simulate_torch_operations(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ece1bb6-85b4-456c-89e3-1d84dbb64d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(batch, model, backend):\n",
    "    \"\"\"\n",
    "    The MLE loss for a batch.\n",
    "\n",
    "    Parameters:\n",
    "    - batch: The result of collate_fn, a dict with \"input_ids\", \"labels\", and \"label_token_weights\".\n",
    "    - model: The model to be trained.\n",
    "\n",
    "    Returns:\n",
    "    - A scalar loss value for this batch, averaged across all target tokens.\n",
    "\n",
    "    # ------------ToDo------------\n",
    "    add preference loss\n",
    "    add preference model\n",
    "    # ------------ToDo------------\n",
    "    \"\"\"\n",
    "\n",
    "    idx = batch['concatenated_input_ids']\n",
    "    idx.requires_grad_(True)\n",
    "    \n",
    "    logits = model(idx=idx)\n",
    "    \n",
    "    # chosen_logits = logits[:len(batch['chosen_input_ids'])]\n",
    "    # rejected_logits = logits[len(batch['chosen_input_ids']):]\n",
    "    \n",
    "    labels = batch['concatenated_labels']\n",
    "\n",
    "    loss_mask = labels != 0\n",
    "\n",
    "  \n",
    "    batch_size, seq_len, vocab_size = logits.shape\n",
    "    \n",
    "    \n",
    "    # print('logits shape: ', logsoftmax(logits, -1).shape)\n",
    "    # print('labels shape: ', labels.shape)\n",
    "\n",
    "    #TODO!!!!!!!!!!!!\n",
    "\n",
    "    # logits = logits.to_numpy()\n",
    "    # labels = labels.to_numpy()\n",
    "    # logits = torch.tensor(logits)\n",
    "    # labels = torch.tensor(labels, dtype=torch.int64)\n",
    "    \n",
    "    # per_token_logps = custom_gather(logsoftmax(logits, -1).detach().to_numpy(), 2, labels.view(batch_size, seq_len, 1).to_numpy())\n",
    "    # per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)\n",
    "    per_token_logps = simulate_torch_operations(logits, labels)\n",
    "    \n",
    "    # print(per_token_logps.shape)\n",
    "    \n",
    "    batch_logps = (per_token_logps * loss_mask).sum(-1)\n",
    "\n",
    "    chosen_length = len(batch_logps) // 2\n",
    "\n",
    "    batch_logps = batch_logps.numpy()\n",
    "\n",
    "    chosen_logps = batch_logps[:chosen_length]\n",
    "    rejected_logps = batch_logps[chosen_length:]\n",
    "\n",
    "    chosen_logps = tensor_from_numpy(chosen_logps, backend)\n",
    "    rejected_logps = tensor_from_numpy(rejected_logps, backend)\n",
    "    \n",
    "    # loss, chosen_rewards, rejected_rewards = minitorch.nn.preference_loss(batch_logps[:chosen_length], batch_logps[chosen_length:], beta=0.7)\n",
    "    loss, chosen_rewards, rejected_rewards = minitorch.nn.preference_loss(chosen_logps, rejected_logps, beta=0.7)\n",
    "\n",
    "    return loss, chosen_rewards, rejected_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a2905b9-8478-401b-811b-ed293409a73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, optimizer, examples, n_samples, collate_fn, batch_size, desc, backend):\n",
    "#     \"\"\"\n",
    "#     Trains the model on the provided examples.\n",
    "\n",
    "#     Parameters:\n",
    "#     - model: The model to be trained.\n",
    "#     - optimizer: The optimizer used for updating the model's parameters.\n",
    "#     - examples: The dataset examples used for training.\n",
    "#     - n_samples: The random samples to train from \"examples\".\n",
    "#     - collate_fn: The function to collate data examples into batches.\n",
    "#     - batch_size: The number of examples in each batch.\n",
    "#     - desc: Description for the training process (used in progress bars).\n",
    "\n",
    "#     # ------------ToDo------------\n",
    "#     add preference policy model\n",
    "#     # ------------ToDo------------\n",
    "#     \"\"\"\n",
    "#     model.train()\n",
    "#     # random.shuffle(examples)\n",
    "#     # examples = examples[:n_samples]\n",
    "\n",
    "#     for i in (prog_bar := trange(\n",
    "#             0, len(examples), batch_size, desc=f'Training ({desc})')):\n",
    "#         batch = collate_fn(examples=examples[i:i + batch_size])\n",
    "\n",
    "#         t0 = time.time()\n",
    "#         optimizer.zero_grad()\n",
    "#         loss, _, _ = loss_fn(batch=batch, model=model, backend=backend)\n",
    "#         t1 = time.time()\n",
    "\n",
    "#         loss.backward()\n",
    "#         t2 = time.time()\n",
    "\n",
    "#         optimizer.step()\n",
    "#         t3 = time.time()\n",
    "\n",
    "#         print(f\"Forward: {t1 - t0}\")\n",
    "#         print(f\"Backward: {t2 - t1}\")\n",
    "#         print(f\"Opt.step: {t3 - t2}\")\n",
    "\n",
    "#         batch_time = time.time() - t0\n",
    "#         prog_bar.set_postfix(\n",
    "#             tokens_per_sec=np.prod(batch['input_ids'].shape) / batch_time,\n",
    "#             loss=loss.item(),\n",
    "#             lr=optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e6741bc4-3391-48b9-959f-df904c644b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, examples, n_samples, collate_fn, batch_size, desc, backend):\n",
    "    \"\"\"\n",
    "    Trains the model on the provided examples.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The model to be trained.\n",
    "    - optimizer: The optimizer used for updating the model's parameters.\n",
    "    - examples: The dataset examples used for training.\n",
    "    - n_samples: The random samples to train from \"examples\".\n",
    "    - collate_fn: The function to collate data examples into batches.\n",
    "    - batch_size: The number of examples in each batch.\n",
    "    - desc: Description for the training process (used in progress bars).\n",
    "\n",
    "    # ------------ToDo------------\n",
    "    add preference policy model\n",
    "    # ------------ToDo------------\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    # random.shuffle(examples)\n",
    "    # examples = examples[:n_samples]\n",
    "\n",
    "    for i in range(0, len(examples), batch_size):\n",
    "        batch = collate_fn(examples=examples[i:i + batch_size])\n",
    "\n",
    "        t0 = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss, _, _ = loss_fn(batch=batch, model=model, backend=backend)\n",
    "        t1 = time.time()\n",
    "\n",
    "        loss.backward()\n",
    "        t2 = time.time()\n",
    "\n",
    "        optimizer.step()\n",
    "        t3 = time.time()\n",
    "\n",
    "        print(f\"Forward: {t1 - t0}\")\n",
    "        print(f\"Backward: {t2 - t1}\")\n",
    "        print(f\"Opt.step: {t3 - t2}\")\n",
    "\n",
    "        batch_time = time.time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe3f925e-b555-4be3-9c9f-2f35ddbb5f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model_max_length=512,\n",
    "         n_epochs=2,\n",
    "         batch_size=10,\n",
    "         learning_rate=0.02,\n",
    "         samples_per_epoch=2,\n",
    "         n_vocab=10000,\n",
    "         n_embd=256,\n",
    "         seed=11111):\n",
    "    \"\"\"\n",
    "    The main function to train and evaluate the model on a specified dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_name: The name of the dataset to be used.\n",
    "    - model_max_length: The maximum sequence length the model can handle.\n",
    "    - n_epochs: The number of training epochs.\n",
    "    - batch_size: The number of examples in each batch.\n",
    "    - learning_rate: The learning rate for the optimizer.\n",
    "    - samples_per_epoch: Samples from the training dataset every epoch.\n",
    "    - n_vocab: The vocabulary size of the BPE tokenizer.\n",
    "    - n_embd: The embedding dimension.\n",
    "    - seed: Random seed.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # workdir = f'./workdir_vocab{n_vocab}_lr{learning_rate}_embd{n_embd}'\n",
    "    # os.makedirs(workdir, exist_ok=True)\n",
    "\n",
    "    backend = minitorch.TensorBackend(CudaKernelOps)\n",
    "\n",
    "    config = {\n",
    "        'n_vocab': 50257,  # vocab_size\n",
    "        'n_embd': 256,  # n_embed\n",
    "        'n_head': 8,  # n_head\n",
    "        'n_positions': 512,  # n_ctx == n_positions\n",
    "        # 'n_layer'     : 4,    # n_layer\n",
    "        'p_dropout': 0.1,  # x_pdrop\n",
    "        'ln_eps': 1e-5,  # layer_norm_epsilon\n",
    "        'backend': backend\n",
    "    }\n",
    "\n",
    "    model = DecoderLM(**config)\n",
    "    optimizer = minitorch.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    dataset = get_imdb()\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2-large', special_tokens={'pad_token': '<pad>'})\n",
    "\n",
    "    collate_fn = partial(collate_batch, tokenizer=tokenizer, model_max_length=512, backend=backend)\n",
    "\n",
    "    for epoch_idx in range(n_epochs):\n",
    "        desc = f'epoch {epoch_idx} / {n_epochs}'\n",
    "\n",
    "        train(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            examples=dataset['train'],\n",
    "            n_samples=samples_per_epoch,\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=collate_fn,\n",
    "            desc=desc,\n",
    "            backend=backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "78711035-4461-4da6-a139-99c4dad1f09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB RLHF dataset...\n",
      "done\n"
     ]
    },
    {
     "ename": "IndexingError",
     "evalue": "Index [0] must be size of (20, 512, 50257).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[48], line 54\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(model_max_length, n_epochs, batch_size, learning_rate, samples_per_epoch, n_vocab, n_embd, seed)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m     52\u001b[0m     desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 54\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msamples_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[41], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, examples, n_samples, collate_fn, batch_size, desc, backend)\u001b[0m\n\u001b[1;32m     25\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 27\u001b[0m loss, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     30\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[47], line 46\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(batch, model, backend)\u001b[0m\n\u001b[1;32m     31\u001b[0m batch_size, seq_len, vocab_size \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# print('logits shape: ', logsoftmax(logits, -1).shape)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# print('labels shape: ', labels.shape)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# per_token_logps = custom_gather(logsoftmax(logits, -1).detach().to_numpy(), 2, labels.view(batch_size, seq_len, 1).to_numpy())\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m per_token_logps \u001b[38;5;241m=\u001b[39m \u001b[43msimulate_torch_operations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# print(per_token_logps.shape)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m batch_logps \u001b[38;5;241m=\u001b[39m (per_token_logps \u001b[38;5;241m*\u001b[39m loss_mask)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[45], line 11\u001b[0m, in \u001b[0;36msimulate_torch_operations\u001b[0;34m(logits, labels)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimulate_torch_operations\u001b[39m(logits, labels):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Assuming logits is a list of lists and labels is a list of indices\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Convert logits to log softmax\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     log_softmax_logits \u001b[38;5;241m=\u001b[39m [logsoftmax(logit, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m logit \u001b[38;5;129;01min\u001b[39;00m logits]\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Gather using the custom gather function\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# labels.unsqueeze(2) equivalent in basic python could be transforming label indices\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     gathered_results \u001b[38;5;241m=\u001b[39m [custom_gather(log_softmax_logit, [label]) \u001b[38;5;28;01mfor\u001b[39;00m log_softmax_logit, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(log_softmax_logits, labels)]\n",
      "Cell \u001b[0;32mIn[45], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimulate_torch_operations\u001b[39m(logits, labels):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Assuming logits is a list of lists and labels is a list of indices\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Convert logits to log softmax\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     log_softmax_logits \u001b[38;5;241m=\u001b[39m [logsoftmax(logit, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m logit \u001b[38;5;129;01min\u001b[39;00m logits]\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Gather using the custom gather function\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# labels.unsqueeze(2) equivalent in basic python could be transforming label indices\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     gathered_results \u001b[38;5;241m=\u001b[39m [custom_gather(log_softmax_logit, [label]) \u001b[38;5;28;01mfor\u001b[39;00m log_softmax_logit, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(log_softmax_logits, labels)]\n",
      "File \u001b[0;32m~/11868/llm_sys_project/minitorch/tensor.py:282\u001b[0m, in \u001b[0;36mTensor.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: Union[\u001b[38;5;28mint\u001b[39m, UserIndex]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    281\u001b[0m     key2 \u001b[38;5;241m=\u001b[39m (key,) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m key\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/11868/llm_sys_project/minitorch/tensor_data.py:240\u001b[0m, in \u001b[0;36mTensorData.get\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: UserIndex) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m     x: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/11868/llm_sys_project/minitorch/tensor_data.py:219\u001b[0m, in \u001b[0;36mTensorData.index\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# Check for errors\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aindex\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IndexingError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be size of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, ind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(aindex):\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ind \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[i]:\n",
      "\u001b[0;31mIndexingError\u001b[0m: Index [0] must be size of (20, 512, 50257)."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f30921c1-cd8f-4ebf-8d0d-5a3b918ab158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     dataset = get_imdb()\n",
    "#     # print(dataset['train'][0]['prompt'])\n",
    "    \n",
    "#     tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2-large', special_tokens={'pad_token': '<pad>'})\n",
    "#     # tokenizer.pad_token = '<pad>'\n",
    "#     # print(tokenizer.eos_token_id)\n",
    "#     batch = tokenize_batch_element(dataset['train'][0]['prompt'], dataset['train'][0]['chosen'], dataset['train'][0]['rejected'], 'keep_start', tokenizer, 512, 256)\n",
    "#     print('chosen_input_ids: ', batch['chosen_input_ids'])\n",
    "    \n",
    "#     print('chosen: ', batch)\n",
    "#     # \n",
    "#     # print('dataset: ', dataset['train'][:10])\n",
    "#     backend = minitorch.TensorBackend(CudaKernelOps)\n",
    "#     batch = collate_batch(dataset['train'][:10], tokenizer=tokenizer, model_max_length=512, backend=backend)\n",
    "    \n",
    "#     config = {\n",
    "#         'n_vocab': 50257,  # vocab_size\n",
    "#         'n_embd': 256,  # n_embed\n",
    "#         'n_head': 8,  # n_head\n",
    "#         'n_positions': 512,  # n_ctx == n_positions\n",
    "#         # 'n_layer'     : 4,    # n_layer\n",
    "#         'p_dropout': 0.1,  # x_pdrop\n",
    "#         'ln_eps': 1e-5,  # layer_norm_epsilon\n",
    "#         'backend': backend\n",
    "#     }\n",
    "#     model = DecoderLM(**config)\n",
    "#     # loss, _, _ = loss_fn(batch, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436a9ae5-80d5-46ac-8338-001028a3fbdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmsys",
   "language": "python",
   "name": "llmsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
